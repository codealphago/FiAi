{
 "metadata": {
  "signature": "sha256:104859f2d54a375e7076a1b1b74b79ee8cb1a9e18baa8968e074fba48df0a879"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "original_thread": "586a4eb889e3ba004defde4b",
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "id": "9C7CA639D6614A1A9740B8289C42AD89",
     "input": [
      "%%time\n",
      "from __future__ import division\n",
      "from __future__ import print_function  \n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pylab as plt\n",
      "%matplotlib inline\n",
      "import seaborn as sns\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "fac = np.load('/home/big/Quotes/TensorFlow deal with Uqer/fac16.npy').astype(np.float32)\n",
      "ret = np.load('/home/big/Quotes/TensorFlow deal with Uqer/ret16.npy').astype(np.float32)\n",
      "# \u6570\u636e\u683c\u5f0f \u65e5\u671f-\u591a\u56e0\u5b50 \u4f8b\u5982 \uff0809-01 Ab1 Ab2 Ab3 \uff09\uff0809-02 Ab1 Ab2 Ab3\uff09 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "id": "8925B0D9E1254FA58A7211E61C27CA58",
     "input": [
      "# Parameters\n",
      "learning_rate = 0.001\n",
      "training_iters = 100000\n",
      "batch_size = 1280\n",
      "display_step = 10\n",
      "\n",
      "# Network Parameters\n",
      "n_input = 17 # MNIST data input (img shape: 28*28)\n",
      "n_steps = 40 # timesteps\n",
      "n_hidden = 128 # hidden layer num of features\n",
      "n_classes = 7 # MNIST total classes (0-9 digits)\n",
      "\n",
      "\n",
      "# tf Graph input\n",
      "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
      "y = tf.placeholder(\"float\", [None, n_classes])\n",
      "\n",
      "# Define weights\n",
      "weights = {\n",
      "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
      "    'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))\n",
      "}\n",
      "biases = {\n",
      "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
      "}\n",
      "\n",
      "def BiRNN(x, weights, biases):\n",
      "\n",
      "    # Prepare data shape to match `bidirectional_rnn` function requirements\n",
      "    # Current data input shape: (batch_size, n_steps, n_input)\n",
      "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
      "\n",
      "    # Permuting batch_size and n_steps\n",
      "    x = tf.transpose(x, [1, 0, 2])\n",
      "    # Reshape to (n_steps*batch_size, n_input)\n",
      "    x = tf.reshape(x, [-1, n_input])\n",
      "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
      "    x = tf.split(0, n_steps, x)\n",
      "\n",
      "    # Define lstm cells with tensorflow\n",
      "    # Forward direction cell\n",
      "    lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
      "    # Backward direction cell\n",
      "    lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
      "\n",
      "    # Get lstm cell output\n",
      "    try:\n",
      "        outputs, _, _ = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
      "                                              dtype=tf.float32)\n",
      "    except Exception: # Old TensorFlow version only returns outputs not states\n",
      "        outputs = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
      "                                        dtype=tf.float32)\n",
      "\n",
      "    # Linear activation, using rnn inner loop last output\n",
      "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
      "pred = BiRNN(x, weights, biases)\n",
      "\n",
      "# Define loss and optimizer\n",
      "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
      "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
      "\n",
      "# Evaluate model\n",
      "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
      "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
      "\n",
      "# Initializing the variables\n",
      "init = tf.global_variables_initializer()\n",
      "\n",
      "# Launch the graph\n",
      "with tf.Session() as sess:\n",
      "    sess.run(init)\n",
      "    step = 1\n",
      "    for step in range(100):\n",
      "        for i in range(int(len(fac)/batch_size)):\n",
      "            batch_x = fac[i*batch_size:(i+1)*batch_size].reshape([batch_size,n_steps,n_input])\n",
      "            batch_y = ret[i*batch_size:(i+1)*batch_size].reshape([batch_size,n_classes])\n",
      "            sess.run(optimizer,feed_dict={x:batch_x,y:batch_y})           \n",
      "            if i % display_step ==0:\n",
      "                print(i,'----',(int(len(fac)/batch_size)))\n",
      "        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,y: batch_y})\n",
      "        print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
      "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
      "                  \"{:.5f}\".format(acc))\n",
      "    print(\"Optimization Finished!\")   \n",
      "    # Calculate accuracy for 128 mnist test images\n",
      "    test_len = 1280\n",
      "    test_data = fac[:test_len].reshape([batch_size,n_steps,n_input])\n",
      "    test_label = ret[:test_len].reshape([batch_size,n_classes])\n",
      "\n",
      "    print(\"Testing Accuracy:\", \\\n",
      "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n",
      "    \n",
      "    sess.close()    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "id": "059172B9591F4F7F82E00A709AD8F13C",
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}